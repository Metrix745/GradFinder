{
    "scholar_id": "HUpnUScAAAAJ",
    "name": "\u202aShimei Pan\u202c - \u202aGoogle Scholar\u202c",
    "description": [
        " It is now widely acknowledged that machine learning models, trained on data without due care, often exhibit discriminatory behavior. Traditional fairness research has mainly focused on supervised learning tasks, particularly classification. While fairness in unsupervised learning has received some attention, the literature has primarily addressed fair representation learning of continuous embeddings. This paper, however, takes a different approach by investigating fairness in unsupervised learning using graphical models with discrete latent variables. We develop a fair stochastic variational inference method for discrete latent variables. Our approach uses a fairness penalty on the variational distribution that reflects the principles of intersectionality, a comprehensive perspective on fairness from the fields of law, social sciences, and humanities. Intersectional fairness brings the challenge of data sparsity in\u00a0\u2026",
        " Achieving fairness in AI systems is a critical yet challenging task due to conflicting metrics and their underlying societal assumptions, e.g., the extent to which racist and sexist societal processes are presumed to cause harm and the extent to which we should apply affirmative corrections. Moreover, these measures often contradict each other and might also make the AI system less accurate. This work takes a step towards a unifying human-centered fairness framework to guide stakeholders in navigating these complexities, including their potential incompatibility and the corresponding trade-offs. Our framework acknowledges the spectrum of fairness definitions \u2014individual vs. group fairness, infra-marginal (politically conservative) vs. intersectional (politically progressive) treatment of disparities\u2014 allowing stakeholders to prioritize desired outcomes by assigning weights to various fairness considerations, trading\u00a0\u2026",
        "Large Language Models (LLMs) are prone to generating content that exhibits gender biases, raising significant ethical concerns. Alignment, the process of fine-tuning LLMs to better align with desired behaviors, is recognized as an effective approach to mitigate gender biases. Although proprietary LLMs have made significant strides in mitigating gender bias, their alignment datasets are not publicly available. The commonly used and publicly available alignment dataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of publicly available alignment datasets specifically designed to address gender bias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating a comprehensive set of gender biases in LLMs. This dataset comprises 8k single-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response. Compared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower levels of gender bias and higher quality. Furthermore, we categorized the gender biases in the \"rejected\" responses of GenderAlign into 4 principal categories. The experimental results show the effectiveness of GenderAlign in reducing gender bias in LLMs.",
        "Speech data has rich acoustic and paralinguistic information with important cues for understanding a speaker's tone, emotion, and intent, yet traditional large language models such as BERT do not incorporate this information. There has been an increased interest in multi-modal language models leveraging audio and/or visual information and text. However, current multi-modal language models require both text and audio/visual data streams during inference/test time. In this work, we propose a methodology for training language models leveraging spoken language audio data but without requiring the audio stream during prediction time. This leads to an improved language model for analyzing spoken transcripts while avoiding an audio processing overhead at test time. We achieve this via an audio-language knowledge distillation framework, where we transfer acoustic and paralinguistic information from a pre-trained speech embedding (OpenAI Whisper) teacher model to help train a student language model on an audio-text dataset. In our experiments, the student model achieves consistent improvement over traditional language models on tasks analyzing spoken transcripts.",
        "Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g., along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when humans and fair AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer career recommendations without sacrificing its accuracy in prediction. Nevertheless, an online user study of more than 200 college\u00a0\u2026",
        "Data Science (DS) is an interdisciplinary topic that is applicable to many domains. In this preliminary investigation, we use caselet, a mini-version of a case study, as a learning tool to allow students to practice data science problem solving (DSPS). Using a dataset collected from a real-world classroom, we performed correlation analysis to reveal the structure of cognition and metacognition processes. We also explored the similarity of different DS knowledge components based on students\u2019 performance. In addition, we built a predictive model to characterize the relationship between metacognition, cognition, and learning gain.",
        "Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from \"hallucinations,\" where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information. We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88% effectiveness.",
        "First defned by NASA in 2010, a digital twin is a synchronized digital replica of a physical system used to monitor, model, and fne-tune the system\u2019s performance throughout its lifetime. Digital twins have been widely used in aerospace engineering, manufacturing, construction, urban planning, and automotive. In healthcare, a digital twin can be used to monitor a person\u2019s health conditions, evaluate potential preventive or therapeutic plans, design personalized treatment, and interventions, prevent potential health issues, and facilitate virtual clinical trials. In principle, the idea of digital twins for health is poised to revolutionize modern healthcare with the help of artifcial intelligence (AI) and machine learning (ML). However, there is still a long way to go before its potential can be fully realized for the beneft of personal health and well-being. One of the main challenges is that the human body is a complex multiscale\u00a0\u2026",
        "We propose definitions of fairness in machine learning and artificial intelligence systems that are informed by the framework of intersectionality, a critical lens from the legal, social science, and humanities literature which analyzes how interlocking systems of power and oppression affect individuals along overlapping dimensions including gender, race, sexual orientation, class, and disability. We show that our criteria behave sensibly for any subset of the set of protected attributes, and we prove economic, privacy, and generalization guarantees. Our theoretical results show that our criteria meaningfully operationalize AI fairness in terms of real-world harms, making the measurements interpretable in a manner analogous to differential privacy. We provide a simple learning algorithm using deterministic gradient methods, which respects our intersectional fairness criteria. The measurement of fairness becomes statistically challenging in the minibatch setting due to data sparsity, which increases rapidly in the number of protected attributes and in the values per protected attribute. To address this, we further develop a practical learning algorithm using stochastic gradient methods which incorporates stochastic estimation of the intersectional fairness criteria on minibatches to scale up to big data. Case studies on census data, the COMPAS criminal recidivism dataset, the HHP hospitalization data, and a loan application dataset from HMDA demonstrate the utility of our methods.",
        " We have developed a set of Python applications that use large language models to identify and analyze data from social media platforms relevant to a population of interest. Our pipeline begins with using OpenAI\u2019s GPT-3 to generate potential keywords for identifying relevant text content from the target population. The keywords are then validated, and the content downloaded and analyzed using GPT-3 embedding and manifold reduction. Corpora are then created to fine-tune GPT-2 models to explore latent information via prompt-based queries. These tools allow researchers and practitioners to gain valuable insights into population subgroups online.",
        "NLP applications are widely used in everyday life: web search, grammar correction, machine translation, chatbots/virtual assistants etc,. They are commonly available on our computers and mobile phones. Moreover, very large pretrained language models such as BERT and GPT-3 are at the core of many applications that understand and generate natural language. Since these models are mostly trained on human-generated data (eg, text from the web/social media), they frequently inherit human biases and prejudices. In this seminar, we will discuss the implications of this. We will answer questions such as \u201cHow can we assess the bias in NLP models and data?\u201d and \u201cHow to debias language models and NLP applications?\u201d Bias assessment and mitigation will be the focus of the first half of the seminar. The second half will be dedicated to dual use: NLP helps not only us, but also e-commerce to get to know more about their customers, the industry to place personalized advertisements, authoritarian governments to censor posts in microblogs and social networks, secret services to search phone calls and emails not only for keywords but also for contents. In the seminar we will look at methods and applications from sentiment analysis, machine translation, text mining, NLP and social media, NLP in health applications, etc. We will question their ethical implications and their impact on society.",
        "With a constant increase of learned parameters, modern neural language models become increasingly more powerful. Yet, explaining these complex model's behavior remains a widely unsolved problem. In this paper, we discuss the role interactive visualization can play in explaining NLP models (XNLP). We motivate the use of visualization in relation to target users and common NLP pipelines. We also present several use cases to provide concrete examples on XNLP with visualization. Finally, we point out an extensive list of research opportunities in this field.",
        "It is now well understood that machine learning models, trained on data without due care, often exhibit unfair and discriminatory behavior against certain populations. Traditional algorithmic fairness research has mainly focused on supervised learning tasks, particularly classification. While fairness in unsupervised learning has received some attention, the literature has primarily addressed fair representation learning of continuous embeddings. In this paper, we conversely focus on unsupervised learning using probabilistic graphical models with discrete latent variables. We develop a fair stochastic variational inference technique for the discrete latent variables, which is accomplished by including a fairness penalty on the variational distribution that aims to respect the principles of intersectionality, a critical lens on fairness from the legal, social science, and humanities literature, and then optimizing the variational parameters under this penalty. We first show the utility of our method in improving equity and fairness for clustering using na\\\"ive Bayes and Gaussian mixture models on benchmark datasets. To demonstrate the generality of our approach and its potential for real-world impact, we then develop a special-purpose graphical model for criminal justice risk assessments, and use our fairness approach to prevent the inferences from encoding unfair societal biases.",
        "Psycholinguistic knowledge resources have been widely used in constructing features for text-based human trait and behavior analysis. Recently, deep neural network (NN)-based text analysis methods have gained dominance due to their high prediction performance. However, NN-based methods may not perform well in low resource scenarios where the ground truth data is limited (eg, only a few hundred labeled training instances are available). In this research, we investigate diverse methods to incorporate Linguistic Inquiry and Word Count (LIWC), a widely-used psycholinguistic lexicon, in NN models to improve human trait and behavior analysis in low resource scenarios. We evaluate the proposed methods in two tasks: predicting delay discounting and predicting drug use based on social media posts. The results demonstrate that our methods perform significantly better than baselines that use only LIWC or only NN-based feature learning methods. They also performed significantly better than published results on the same dataset.",
        "When a human receives a prediction or recommended course of action from an intelligent agent, what additional information, beyond the prediction or recommendation itself, does the human require from the agent to decide whether to trust or reject the prediction or recommendation? In this paper we survey literature in the area of trust between a single human supervisor and a single agent subordinate to determine the nature and extent of this additional information and to characterize it into a taxonomy that can be leveraged by future researchers and intelligent agent practitioners. By examining this question from a human-centered, information-focused point of view, we can begin to compare and contrast different implementations and also provide insight and directions for future work.",
        "This discussion focused on definitions and categorization of bias (eg, social bias, system bias, cognitive bias, and sample bias) and methods to identify and mitigate all in the context of text analysis and visualization. We (Figure 26) discussed the data processing pipelines from the NLP community and the data visualization community as a lens through which to discuss areas where bias can appear. A fundamental point when talking about bias is that biases can be found or introduced in every step of the pipeline. Locating bias in the pipeline can be a challenge. There can be bias in the data, this can be amplified or even introduced by the model, by choices on how to visualize the data, the transformations of the data as part of the visualization and in the eye of the beholder interpreting the results.",
        " Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g. along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when human- fair-AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer and more accurate college major recommendations. Nevertheless, an online user study of more than 200 college students revealed that\u00a0\u2026",
        "A method includes generating, as executed by a processor on a computer, a plurality of topic-specific user knowledge models for each user of a plurality of users, each topic-specific user knowledge model representing a level of knowledge possessed by a respective user on a single topic from a set of globally defined topics shared among the plurality of users, generating a plurality of topic-specific expert knowledge models, each topic-specific expert knowledge model representing an aggregate level of knowledge possessed by a plurality of expert users on a single topic from a set of globally defined topics shared among the plurality of users, comparing the topic-specific user knowledge model of the first user with the topic-specific expert knowledge model for a respective topic to determine a distance between a user knowledge level and an aggregate expert knowledge level for the topic.",
        "A system includes a user model module that generates a plurality of topic-specific user knowledge models for each user of a plurality of users, each topic-specific user knowledge model representing a level of knowledge possessed by a respective user on a single topic from a set of globally defined topics shared among the plurality of users, a expertise model building module that generates a plurality of topic-specific expert knowledge models, each topic-specific expert knowledge model representing an aggregate level of knowledge possessed by a plurality of expert users on a single topic from a set of globally defined topics shared among the plurality of users, and a processor of a computer that executes instructions for comparing the topic-specific user knowledge model of the first user with the topic-specific expert knowledge model for a respective topic to determine a distance between a user knowledge level\u00a0\u2026",
        "In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (eg, Wikipedia versus clinic notes), these models may not be ideal for domain-specific tasks (eg, extracting clinical relations). Furthermore, it may require additional medical knowledge to understand clinical text properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add medical knowledge into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2/VA 2010 clinical relation extraction dataset."
    ],
    "title": [
        "\u202aFair Inference for Discrete Latent Variable Models: An Intersectional Approach\u202c",
        "\u202aTowards A Unifying Human-Centered AI Fairness Framework\u202c",
        "\u202aGenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models\u202c",
        "\u202aTeach me with a Whisper: Enhancing Large Language Models for Analyzing Spoken Transcripts using Speech Embeddings\u202c",
        "\u202aWhen Biased Humans Meet Debiased AI: A Case Study in College Major Recommendation\u202c",
        "\u202aModeling metacognitive and cognitive processes in data science problem solving (student abstract)\u202c",
        "\u202aTrapping llm hallucinations using tagged context prompts\u202c",
        "\u202aInternational Workshop on Digital Twins for Smart Health\u202c",
        "\u202aDifferential fairness: an intersectional framework for fair AI\u202c",
        "\u202aThe Keyword Explorer Suite: A Toolkit for Understanding Online Populations\u202c",
        "\u202aEthics in nlp: Bias and dual use\u202c",
        "\u202aThe role of interactive visualization in explaining (large) NLP models: from data to inference\u202c",
        "\u202aFair inference for discrete latent variable models\u202c",
        "\u202aIncorporating LIWC in neural networks to improve human trait and behavior analysis in low resource scenarios\u202c",
        "\u202aTell me something that will help me trust you: A survey of trust calibration in human-agent interaction\u202c",
        "\u202a6.5 WG: Bias and Bias Mitigation\u202c",
        "\u202aDo humans prefer debiased AI algorithms? A case study in career recommendation\u202c",
        "\u202aSystems, methods, and computer program products for expediting expertise\u202c",
        "\u202aSystems, methods, and computer program products for expediting expertise\u202c",
        "\u202aIncorporating medical knowledge in BERT for clinical relation extraction\u202c"
    ]
}